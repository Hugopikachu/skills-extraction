{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b30d5c84-00b9-4129-bfd8-7c5027ee7e49",
   "metadata": {},
   "source": [
    "# Playground\n",
    "\n",
    "A notebook for experimenting and playing with the data before putting everything together.\n",
    "\n",
    "This notebook transcribes the successive steps and evolutions of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770003f2-8c03-461e-8c91-c2592037854d",
   "metadata": {},
   "source": [
    "1) [Imports](#Imports)\n",
    "2) [Collecting Data](#Collecting-data)\n",
    "3) [Extracting Data](#Data-extraction-and-generation)\n",
    "4) [BOW](#Bag-Of-Words)\n",
    "    - [From Scratch](#From-scratch)\n",
    "    - [With SKLearn](#Using-Scikit-Learn)\n",
    "    - [With Stemming](#Now-with-stemming)\n",
    "5) [Tf-Idf](#TF-IDF)\n",
    "    - [Basic Use](#Basic-use)\n",
    "    - [With Stemming](#Adding-stemming)\n",
    "    - [Some Improvements ?](#Some-improvements-?)\n",
    "6) [Word2Vec](#Word2Vec)\n",
    "    - [Basic Use](#Basic)\n",
    "    - [With Stemming](#With-Stemming)\n",
    "    - [With Tf-Idf](#Adding-Tf-Idf)\n",
    "7) [Spacy](#Spacy-and-pre-trained-models)\n",
    "8) [PDF Extraction](#Extracting-skills-from-a-resume)\n",
    "    - [Best Matches Per Sentence](#Extract-best-skill-per-sentence)\n",
    "    - [Best Matches Overall](#Extract-best-skills-overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b767ca3c-ce92-42ca-9ebe-e06388caf60b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0047b5af-0c31-4821-9034-546f0e84d4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/splash/Projects/NLP_resume/venv/lib/python3.9/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/splash/Projects/NLP_resume/venv/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from re import split, sub\n",
    "from pathlib import Path\n",
    "from sys import maxunicode\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "from unicodedata import category\n",
    "from unidecode import unidecode\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941d6066-da3b-48c1-b53c-046dcf8e8b4b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Collecting data\n",
    "\n",
    "The list of all job titles and their corresponding data sheets are downloaded from [here](https://www.pole-emploi.fr/files/live/sites/PE/files/ROME_ArboPrincipale.xlsx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf5c7f8-ad7e-41ab-9463-943c4440fd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>famille</th>\n",
       "      <th>domaine</th>\n",
       "      <th>fiche</th>\n",
       "      <th>titre</th>\n",
       "      <th>code_ogr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Agriculture et Pêche, Espaces naturels et Espa...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>Engins agricoles et forestiers</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>11</td>\n",
       "      <td>01</td>\n",
       "      <td>Conduite d'engins agricoles et forestiers</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>11</td>\n",
       "      <td>01</td>\n",
       "      <td>Chauffeur / Chauffeuse de machines agricoles</td>\n",
       "      <td>11987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>11</td>\n",
       "      <td>01</td>\n",
       "      <td>Conducteur / Conductrice d'abatteuses</td>\n",
       "      <td>12862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A</td>\n",
       "      <td>11</td>\n",
       "      <td>01</td>\n",
       "      <td>Conducteur / Conductrice d'automoteur de récolte</td>\n",
       "      <td>38874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A</td>\n",
       "      <td>11</td>\n",
       "      <td>01</td>\n",
       "      <td>Conducteur / Conductrice d'engins d'exploitati...</td>\n",
       "      <td>13254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  famille domaine fiche                                              titre  \\\n",
       "0       A                Agriculture et Pêche, Espaces naturels et Espa...   \n",
       "1       A      11                           Engins agricoles et forestiers   \n",
       "2       A      11    01          Conduite d'engins agricoles et forestiers   \n",
       "3       A      11    01       Chauffeur / Chauffeuse de machines agricoles   \n",
       "4       A      11    01              Conducteur / Conductrice d'abatteuses   \n",
       "5       A      11    01   Conducteur / Conductrice d'automoteur de récolte   \n",
       "6       A      11    01  Conducteur / Conductrice d'engins d'exploitati...   \n",
       "\n",
       "  code_ogr  \n",
       "0           \n",
       "1           \n",
       "2           \n",
       "3    11987  \n",
       "4    12862  \n",
       "5    38874  \n",
       "6    13254  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_data = pd.read_excel(\n",
    "    \"https://www.pole-emploi.fr/files/live/sites/PE/files/ROME_ArboPrincipale.xlsx\",\n",
    "    sheet_name=\"Arbo Principale 16-12-2019\",\n",
    "    header=0,\n",
    "    names=[\"famille\", \"domaine\", \"fiche\", \"titre\", \"code_ogr\"]\n",
    ")\n",
    "job_data.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29995fc6-f967-4378-931e-788ec7785cdc",
   "metadata": {},
   "source": [
    "The letter in the first column (from A -> N) represents the job family.\\\n",
    "For instance : *A -> Agriculture et Pêche, Espaces naturels et Espaces verts, Soins aux animaux*\n",
    "\n",
    "This letter and two digits number in the second column identify the professional field.\\\n",
    "Here : *A11 -> Engins agricoles et forestiers*\n",
    "\n",
    "The sequence built from the first three columns refers to a specific job title and is called the ROME code.\\\n",
    "On the third row : *A1101 -> Conduite d'engins agricoles et forestiers*\n",
    "\n",
    "The other rows prefixed with the same code simply list other job titles that fit the same job description.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08b610d-6131-49d9-abe5-7434e9f0ee01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code_rome</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>Agriculture et Pêche, Espaces naturels et Espa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A11</th>\n",
       "      <td>Engins agricoles et forestiers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1101</th>\n",
       "      <td>Conduite d'engins agricoles et forestiers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A12</th>\n",
       "      <td>Espaces naturels et espaces verts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1201</th>\n",
       "      <td>Bûcheronnage et élagage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       titre\n",
       "code_rome                                                   \n",
       "A          Agriculture et Pêche, Espaces naturels et Espa...\n",
       "A11                           Engins agricoles et forestiers\n",
       "A1101              Conduite d'engins agricoles et forestiers\n",
       "A12                        Espaces naturels et espaces verts\n",
       "A1201                                Bûcheronnage et élagage"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rome = (job_data\n",
    " .query(\"code_ogr == ' '\")\n",
    " .assign(code_rome=lambda d: (d.famille + d.domaine + d.fiche).str.strip())\n",
    " .drop(columns=[\"famille\", \"domaine\", \"fiche\", \"code_ogr\"])\n",
    " .set_index(\"code_rome\")\n",
    ")\n",
    "rome.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d8af1c-a1cc-470c-831b-cd237407b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ROME codes and their designation in a JSON file for easier use later\n",
    "rome.titre.to_json(\"data/fiches_rome.json\", orient=\"index\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a6d269-7f65-4a1a-a4b8-7a84108e1abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/fiches_rome.json\") as f:\n",
    "    rome_codes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14735c02-f6dc-424f-9a45-061d53267349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many complete ROME codes have we got ? (with 5 characters)\n",
    "len(list(filter(lambda txt: len(txt) == 5, rome_codes.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faac6b7a-49f0-4f92-bc17-10d1078b69fd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "For each job listed with a ROME code, we will fetch useful information from the website [pôle-emploi.fr](https://www.pole-emploi.fr/)\n",
    "such as job aliases, job roles as well as expertises and skills required.\n",
    "\n",
    "We can use the page associated with the first ROME code [here](https://candidat.pole-emploi.fr/marche-du-travail/fichemetierrome?codeRome=A1101) for calibration and for automation of the process, and then apply the same steps to others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "231c7aa5-cd84-419e-bb7b-b9a82683178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://candidat.pole-emploi.fr/marche-du-travail/fichemetierrome\"\n",
    "payload = {\"codeRome\": \"A1101\"}\n",
    "\n",
    "# Query the URl and raise error if the page couldn't be fetched\n",
    "response = get(base_url, params=payload)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Get html and parse it\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f3f6a34-9436-4227-b5bb-676cc579560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the different titles for the job and the roles associated\n",
    "titles, roles = soup.select(\"#js-tabs-unit1-body > .bd\", limit=2)\n",
    "titles = [tag.text for tag in titles.find_all(\"li\")]\n",
    "roles = [tag.text for tag in roles.find_all(\"li\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e85b207-eb0c-4df5-b386-69e9844a6e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the second tab, split for base aptitudes and extra aptitudes\n",
    "base, extra = soup.select(\"#js-tabs-unit2-body > .bd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29f15dff-8e4b-4ee5-91a7-edb7f74017f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we separate skills from expertises using the columns\n",
    "base_expertises = [li.text for li in base.select(\"tr > td:nth-child(odd) li\")]\n",
    "base_skills = [li.text for li in base.select(\"tr > td:nth-child(even) li\")]\n",
    "extra_expertises = [li.text for li in extra.select(\"tr > td:nth-child(odd) li\")]\n",
    "extra_skills = [li.text for li in extra.select(\"tr > td:nth-child(even) li\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a337bb45-b1cf-4865-a390-57b023ce1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_sheet = {\n",
    "    \"appelations\": titles,\n",
    "    \"description\": roles,\n",
    "    \"expertises\": base_expertises,\n",
    "    \"competences\": base_skills,\n",
    "    \"expertises_extra\": extra_expertises,\n",
    "    \"competences_extra\": extra_skills\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eefec4e-c156-4b96-b589-102b3af0bfe4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'appelations': ['Chauffeur / Chauffeuse de machines agricoles',\n",
       "  \"Conducteur / Conductrice d'abatteuses\",\n",
       "  \"Conducteur / Conductrice d'automoteur de récolte\",\n",
       "  \"Conducteur / Conductrice d'engins d'exploitation agricole\",\n",
       "  \"Conducteur / Conductrice d'engins d'exploitation forestière\",\n",
       "  \"Conducteur / Conductrice d'engins de débardage\",\n",
       "  \"Conducteur / Conductrice d'engins forestiers\",\n",
       "  \"Conducteur / Conductrice d'épareuse\",\n",
       "  'Conducteur / Conductrice de machines à vendanger',\n",
       "  'Conducteur / Conductrice de matériels de semis',\n",
       "  'Conducteur / Conductrice de pulvérisateur',\n",
       "  \"Conducteur / Conductrice de tête d'abattage\",\n",
       "  'Conducteur / Conductrice de tracteur',\n",
       "  'Conducteur / Conductrice de tracteur enjambeur',\n",
       "  'Conducteur / Conductrice de tracto-benne',\n",
       "  'Débardeur / Débardeuse',\n",
       "  'Débardeur forestier / Débardeuse forestière',\n",
       "  \"Opérateur / Opératrice d'abatteuse\",\n",
       "  \"Opérateur / Opératrice d'épandage\",\n",
       "  \"Pilote de machines d'abattage\",\n",
       "  'Tractoriste agricole',\n",
       "  'Tractoriste forestier / Tractoriste forestière'],\n",
       " 'description': [\"Réalise des travaux mécanisés agricoles, sylvicoles ou forestiers (préparation des sols, semis, récolte, abattage d'arbres, ...) selon les objectifs de production (quantité, qualité, ...), la commande du client, les règles d'hygiène, de sécurité et la réglementation environnementale.\"],\n",
       " 'expertises': ['Préparer le matériel, les matériaux et les outillages',\n",
       "  \"Identifier le type d'intervention\",\n",
       "  'Sécuriser un équipement',\n",
       "  'Atteler un équipement',\n",
       "  'Conduire les engins agricoles vers le lieu de production (champs, forêt), de stockage, de livraison (fermes, coopératives)',\n",
       "  'Matérialiser une implantation de parcelle ou de passage',\n",
       "  \"Approvisionner des engins d'exploitation\",\n",
       "  'Réaliser une opération de maintenance'],\n",
       " 'competences': ['Procédures de maintenance de matériel',\n",
       "  'Procédures de maintenance de locaux',\n",
       "  'Pneumatique',\n",
       "  'Utilisation de système informatique (embarqué ou fixe)',\n",
       "  \"Utilisation d'engins forestiers\",\n",
       "  'Mécanique',\n",
       "  'Techniques culturales',\n",
       "  'Techniques de préservation de la biodiversité',\n",
       "  'Biologie végétale',\n",
       "  'Engins agricoles',\n",
       "  'Règles de sécurité',\n",
       "  \"Utilisation d'outillages manuels\",\n",
       "  'Techniques de soudage',\n",
       "  'Gestes et postures de manutention',\n",
       "  'Normes environnementales',\n",
       "  'Agronomie',\n",
       "  'Hydraulique'],\n",
       " 'expertises_extra': ['Utiliser un engin nécessitant une habilitation',\n",
       "  'Stocker un produit',\n",
       "  'Débarder une grume',\n",
       "  'Empiler des grumes',\n",
       "  'Récolter un produit à maturité selon les consignes de calibrage',\n",
       "  'Abattre un arbre',\n",
       "  'Débiter un arbre selon son usage',\n",
       "  'Préparer les sols et les plantations (épandage, semis, récolte, ...)',\n",
       "  'Entretenir un espace extérieur',\n",
       "  'Créer et entretenir des pistes',\n",
       "  \"Réaliser l'entretien du matériel\"],\n",
       " 'competences_extra': ['Engins compacts (CACES R 482-A) - A partir du 01/01/2020',\n",
       "  'Engins d’extraction à déplacement séquentiel (CACES R 482-B1) - A partir du 01/01/2020',\n",
       "  'Engins de sondage ou de forage à déplacement séquentiel (CACES R 482-B2) - A partir du 01/01/2020',\n",
       "  'Engins rail-route à déplacement séquentiel (CACES R482-B3) - A partir du 01/01/2020',\n",
       "  'Engins de transport (CACES R 482-E) - A partir du 01/01/2020',\n",
       "  'Chariots de manutention tout-terrain (CACES R 482-F) - A partir du 01/01/2020',\n",
       "  'Chariots à mât rétractable (CACES R 489-5) - A partir du 01/01/2020',\n",
       "  'Grues de chargement (CACES R 490) - A partir du 01/01/2020',\n",
       "  'Tracteur et petits engins de chantiers mobiles (CACES R 372-1)',\n",
       "  \"Engins d'extraction ou de chargement à déplacement séquentiel (CACES R 372-2)\",\n",
       "  \"Engins de transport ou d'extraction transport (CACES R 372-8)\",\n",
       "  'Engins de manutention, chariot de chantier (CACES R 372-9)',\n",
       "  'Chariots élévateurs à mât rétractable (CACES R 389-5)',\n",
       "  'Grues auxiliaires de chargement de véhicules (CACES R 390)',\n",
       "  'Caractéristiques des écosystèmes',\n",
       "  'Utilisation de débusqueur',\n",
       "  'Techniques de pressage',\n",
       "  'Techniques de moissonnage',\n",
       "  'Techniques de fauchage']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4097a8d8-c34c-4527-b405-7b34e2e3c0c2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "It works, let's automate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46381830-f71d-4922-bb40-978b310508be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "532it [02:54,  3.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for rome_code in tqdm(filter(lambda txt: len(txt) == 5, rome_codes.keys())):\n",
    "    # Query the URl and raise error if the page couldn't be fetched\n",
    "    response = get(base_url, params={\"codeRome\": rome_code})\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Get HTML and parse it\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the different titles for the job and the roles associated from the first tab\n",
    "    titles, roles = soup.select(\"#js-tabs-unit1-body > .bd\", limit=2)\n",
    "    titles = [tag.text for tag in titles.find_all(\"li\")]\n",
    "    roles = [tag.text for tag in roles.find_all(\"li\")]\n",
    "    \n",
    "    # On the second tab, split for base aptitudes and extra aptitudes\n",
    "    base, extra = soup.select(\"#js-tabs-unit2-body > .bd\")\n",
    "    # Then we separate skills from knowledges using the columns\n",
    "    base_expertises = [li.text for li in base.select(\"tr > td:nth-child(odd) li\")]\n",
    "    base_skills = [li.text for li in base.select(\"tr > td:nth-child(even) li\")]\n",
    "    extra_expertises = [li.text for li in extra.select(\"tr > td:nth-child(odd) li\")]\n",
    "    extra_skills = [li.text for li in extra.select(\"tr > td:nth-child(even) li\")]\n",
    "    \n",
    "    # Eventually save the job description as JSON in the appropriate sub-directory\n",
    "    subdir = Path(f\"./data/{rome_code[0]}\")\n",
    "    if not subdir.is_dir() :\n",
    "        subdir.mkdir()\n",
    "    with subdir.joinpath(f\"{rome_code}.json\").open(\"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"appelations\": titles,\n",
    "                \"description\": roles,\n",
    "                \"expertises\": base_expertises,\n",
    "                \"competences\": base_skills,\n",
    "                \"expertises_extra\": extra_expertises,\n",
    "                \"competences_extra\": extra_skills\n",
    "            },\n",
    "            f\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb582f-990e-4eb0-804a-313ac59d45ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## Data extraction and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5b7ab8a-e924-43f9-8415-f9d91db37dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    __punctuation_table = {char: \" \" for char in range(maxunicode) if category(chr(char)).startswith(\"P\")}\n",
    "    __stopwords = set(Path(\"stopwords-fr.txt\").read_text().split(\"\\n\"))\n",
    "    __stemmer = FrenchStemmer(ignore_stopwords=True)\n",
    "    \n",
    "    def __init__(self, sentences: list, remove_accents: bool = True, stemming: bool = False):\n",
    "        self.sentences = list(set(sentences))\n",
    "        self.corpus = list()\n",
    "        self.vocab = set()\n",
    "        self.remove_accents = remove_accents\n",
    "        self.stemming = stemming\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return (s for s in self.corpus)\n",
    "    \n",
    "    @property\n",
    "    def tokenized_documents(self):\n",
    "        return [s.split() for s in self.corpus]\n",
    "        \n",
    "    def clean_sentence(self, sentence: str):\n",
    "        sentence = sentence.lower().translate(self.__punctuation_table)\n",
    "        words = [word for word in sentence.split() if word not in self.__stopwords]\n",
    "        if self.stemming:\n",
    "            words = [self.__stemmer.stem(word) for word in words]\n",
    "        if self.remove_accents:\n",
    "            words = [unidecode(word) for word in words]\n",
    "        return \" \".join(words)\n",
    "        \n",
    "    def build_corpus(self):\n",
    "        for i, sentence in enumerate(self.sentences):\n",
    "            clean_sentence = self.clean_sentence(sentence)\n",
    "            self.corpus.append(clean_sentence)\n",
    "            self.vocab.update(clean_sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0762abf-ade3-48c5-8a29-3d86407af732",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('./data/')\n",
    "fiches = []\n",
    "for file in path.glob(\"?/?????.json\"):\n",
    "    with file.open() as j:\n",
    "        fiches.append(json.load(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cec03c37-4934-459c-a211-0bf63a74110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract skills\n",
    "skills = list(chain(*[fiche.get(\"competences\", []) for fiche in fiches], *[fiche.get(\"competences_extra\", []) for fiche in fiches]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfdcee60-8b11-4823-840f-700cffa54a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without stemming\n",
    "corpus = Corpus(skills)\n",
    "corpus.build_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eed55db6-cc8a-463a-a04c-01a19d5f67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with stemming\n",
    "corpus_stem = Corpus(skills, stemming=True)\n",
    "corpus_stem.build_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6071a7-f5b9-4ecc-9a6a-acdba5c57b18",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Bag Of Words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07a9bf7-0234-487b-b5d4-b564b20360c3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### From scratch\n",
    "\n",
    "We can extract a BOW matrix from our corpus object and use it to compute similarities, because why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e07c84b-3dd0-4e4d-b564-5579a08e8cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty array\n",
    "bow = np.empty((len(corpus.corpus), len(corpus.vocab)))\n",
    "\n",
    "# populate it by counting known words for each document of the corpus\n",
    "for i, doc in enumerate(corpus):\n",
    "    words = doc.split()\n",
    "    bow[i] = [words.count(v) for v in sorted(corpus.vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc2e6787-b650-4cb5-a192-341790835ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('techniques', 657.0),\n",
       " ('utilisation', 251.0),\n",
       " ('caracteristiques', 207.0),\n",
       " ('procedures', 141.0),\n",
       " ('pratique', 116.0),\n",
       " ('produits', 103.0),\n",
       " ('01', 80.0),\n",
       " ('gestion', 76.0),\n",
       " ('caces', 71.0),\n",
       " ('logiciel', 71.0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word frequencies\n",
    "word_counts = sorted(list(zip(sorted(corpus.vocab), np.asarray(bow.sum(axis=0)).flatten())), key=lambda e: e[1], reverse=True)\n",
    "word_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "188f6a2e-b2bf-420c-81ee-0edb5da74447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a new sentence\n",
    "skill = \"analyser des comptes clients\"\n",
    "\n",
    "# cleaning it to match corpus vacobulary\n",
    "clean_skill = corpus.clean_sentence(skill)\n",
    "\n",
    "# vectorizing it\n",
    "vec = np.array([clean_skill.split().count(v) for v in sorted(corpus.vocab)])\n",
    "\n",
    "# computing cosine distance\n",
    "cosines = (bow @ vec.T) / np.linalg.norm(bow, axis=1) / np.linalg.norm(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "744dcc43-efb0-496a-980c-cdbfbe060e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gestion des comptes clients', 0.816496580927726)\n",
      "('Révision légale des comptes', 0.408248290463863)\n",
      "('Logiciel de gestion clients', 0.408248290463863)\n",
      "('Gestion commerciale, relation clients', 0.35355339059327373)\n",
      "('Fabrication de fromage', 0.0)\n",
      "('Finance', 0.0)\n",
      "('Langue étrangère - Japonais', 0.0)\n",
      "('Utilisation de pistolet hydraulique', 0.0)\n",
      "(\"Règles d'affranchissement du courrier\", 0.0)\n",
      "('Logiciel Cinéma 4D', 0.0)\n"
     ]
    }
   ],
   "source": [
    "# print 10 most relevant skills from the corpus\n",
    "print(*list(sorted(zip(corpus_stem.sentences, cosines), key=lambda s: s[1], reverse=True))[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75120a01-c092-47e4-8c38-cddc21a26d1b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8813468f-69a2-4492-bf01-9b8960759224",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(lowercase=False, vocabulary=corpus.vocab)\n",
    "bow = cvec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e81f0f88-fdb3-46ba-a523-fc804d0ea37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('techniques', 657),\n",
       " ('utilisation', 251),\n",
       " ('caracteristiques', 207),\n",
       " ('procedures', 141),\n",
       " ('pratique', 116),\n",
       " ('produits', 103),\n",
       " ('01', 80),\n",
       " ('gestion', 76),\n",
       " ('caces', 71),\n",
       " ('logiciel', 71)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = sorted(list(zip(cvec.vocabulary_, np.asarray(bow.sum(axis=0)).flatten())), key=lambda e: e[1], reverse=True)\n",
    "word_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb117103-658b-4cb5-afcc-d72258a347a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cosine similarity with sklearn \n",
    "skill = \"tailler et débiter des arbres\"\n",
    "vec = cvec.transform([corpus.clean_sentence(skill)])\n",
    "cosines = cosine_similarity(bow, vec).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd6e4b85-590e-49c9-887c-a456ece10f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Techniques de soins aux arbres ou ceps', 0.5)\n",
      "('Fabrication de fromage', 0.0)\n",
      "('Finance', 0.0)\n",
      "('Langue étrangère - Japonais', 0.0)\n",
      "('Utilisation de pistolet hydraulique', 0.0)\n",
      "(\"Règles d'affranchissement du courrier\", 0.0)\n",
      "('Logiciel Cinéma 4D', 0.0)\n",
      "('Mécanique des fluides', 0.0)\n",
      "('Procédures de contrôle des matériels et équipements', 0.0)\n",
      "('Caractéristiques des Modulateurs Démodulateurs -MODEM-', 0.0)\n"
     ]
    }
   ],
   "source": [
    "# print 10 most relevant skills from the corpus\n",
    "print(*list(sorted(zip(corpus.sentences, cosines), key=lambda s: s[1], reverse=True))[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bae25f-c423-40d9-9120-c4331c3933c9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Now with stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "073386ae-53f3-4d4f-8624-e4ffa4683514",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(lowercase=False, vocabulary=corpus_stem.vocab)\n",
    "bow = cvec.fit_transform(corpus_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a9943c5-4c53-42ad-a60c-41b7c3844cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cosine similarity with sklearn \n",
    "skill = \"analyser des comptes clients\"\n",
    "vec = cvec.transform([corpus_stem.clean_sentence(skill)])\n",
    "cosines = cosine_similarity(bow, vec).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dfdd677-8f6e-4c76-b47f-8fdc830bf061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gestion des comptes clients', 0.6666666666666669)\n",
      "('Typologie du client', 0.408248290463863)\n",
      "('Analyse de la performance', 0.408248290463863)\n",
      "('Analyse transactionnelle', 0.408248290463863)\n",
      "('Comptabilité client', 0.408248290463863)\n",
      "('Analyse spatiale', 0.408248290463863)\n",
      "('Analyse des coûts', 0.408248290463863)\n",
      "('Analyse médicale', 0.408248290463863)\n",
      "(\"Analyse d'incidents\", 0.408248290463863)\n",
      "('Analyse financière', 0.408248290463863)\n"
     ]
    }
   ],
   "source": [
    "# print 10 most relevant skills from the corpus\n",
    "print(*list(sorted(zip(corpus_stem.sentences, cosines), key=lambda s: s[1], reverse=True))[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f90aa2-12da-444c-b575-6110ae091ceb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##  TF-IDF\n",
    "\n",
    "Some words seem to be frequently used in skill descriptions and to convey less meaning than others.\n",
    "\n",
    "Using Tf-Idf may result in better matching of skills by avoiding giving importance to meaningless words\n",
    "such as _\"réaliser\"_, _\"identifier\"_, _\"définir\"_, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7649d82-d6b5-4336-9bb5-24a5a4f5a1a0",
   "metadata": {},
   "source": [
    "### Basic use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2805160d-b19e-407f-9ed7-1d993b3a3f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=False, vocabulary=corpus.vocab)\n",
    "tf_mat = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "490ae3a3-d6d6-4358-8ee3-9a75dc68665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cosine similarity with sklearn \n",
    "skill = \"analyser des indicateurs financiers\"\n",
    "vec = tfidf.transform([corpus.clean_sentence(skill)])\n",
    "cosines = cosine_similarity(tf_mat, vec).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b176d277-3ce5-4ad3-b698-896da9b8f8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Analyse d'indicateurs financiers\", 0.878542070237204)\n",
      "('Logiciels financiers', 0.5417875419289122)\n",
      "('Caractéristiques des produits financiers', 0.5219383668598823)\n",
      "('Indicateurs statistiques', 0.47898929924549)\n",
      "('Analyse des risques financiers', 0.4594240849173993)\n",
      "('Ratios financiers', 0.43587466497709054)\n",
      "('Calculs financiers', 0.43587466497709054)\n",
      "('Indicateurs de suivi de production', 0.4296576662039001)\n",
      "('Indicateurs de couverture de risques', 0.4266050340826602)\n",
      "('Réglementation des marchés financiers', 0.42142974089207774)\n"
     ]
    }
   ],
   "source": [
    "# print 10 most relevant skills from the corpus\n",
    "print(*list(sorted(zip(corpus_stem.sentences, cosines), key=lambda s: s[1], reverse=True))[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece81045-2060-4864-9c07-4958aeace849",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Adding stemming\n",
    "\n",
    "Let's try again with stemming !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a7d1612-16a6-4923-93a3-5eb2c6f3bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=False, vocabulary=corpus_stem.vocab)\n",
    "tf_mat = tfidf.fit_transform(corpus_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12932d2c-5a46-4ccf-974a-f5c80ccdd855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same process\n",
    "skill = \"administrer des bases de données relationnelles\"\n",
    "vec = tfidf.transform([corpus_stem.clean_sentence(skill)])\n",
    "cosines = cosine_similarity(tf_mat, vec).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "408239ed-2faa-47d1-b225-424ae7377a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gestion de bases de données', 0.5766968689723593)\n",
      "('Logiciels de gestion de base de données', 0.5304828844131672)\n",
      "('Marketing relationnel', 0.4622313004659922)\n",
      "('Système de Gestion de Bases de Données (SGBD)', 0.42104752176086624)\n",
      "('Gestion administrative', 0.3910602315946875)\n",
      "('Données de contrôle', 0.3814266823299183)\n",
      "('Droit administratif', 0.37572217209252623)\n",
      "('Administration centrale', 0.31730902176957465)\n",
      "('Procédures et circuits administratifs', 0.30906172994012265)\n",
      "('Gestion administrative du personnel', 0.2910663892201751)\n"
     ]
    }
   ],
   "source": [
    "# and the top 10 results\n",
    "print(*list(sorted(zip(corpus_stem.sentences, cosines), key=lambda s: s[1], reverse=True))[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9cf866-6a6f-4591-8add-ca7a052a019e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Some improvements ?\n",
    "\n",
    "To go further, we can use tf-idf on letter-grams instead of entire words, \n",
    "it may perform better on resumes with spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3f2a2f0-8f26-47d9-bcac-c001d5dc6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set analyzer to 'char_wb' to specify the use of characters instead of words\n",
    "# and to create only n-grams for chars between the same word boundaries ('wb')\n",
    "# i.e chars belonging to the same word\n",
    "tfidf = TfidfVectorizer(lowercase=False, analyzer=\"char_wb\", ngram_range=(5, 7))\n",
    "tf_mat = tfidf.fit_transform(corpus_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "591d1bbe-0fc1-43f7-be2c-62afc2895b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same process\n",
    "skill = \"créer des bases de données relationnelles\"\n",
    "vec = tfidf.transform([corpus_stem.clean_sentence(skill)])\n",
    "cosines = cosine_similarity(tf_mat, vec).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4174e8d-a8cc-4943-8045-e1dd4328ddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marketing relationnel', 0.7241707133205934)\n",
      "('Progiciels de gestion de la relation client (CRM - Customer Relationship Management)', 0.3348109810201438)\n",
      "('Veille informationnelle', 0.3068995110913636)\n",
      "('Exploration fonctionnelle', 0.20860583904152563)\n",
      "('Pathologies fonctionnelles', 0.19712503021845362)\n",
      "('Normes rédactionnelles', 0.19564125213863048)\n",
      "('Techniques de montage traditionnel', 0.18674828945372202)\n",
      "(\"Méthodes d'analyse (systémique, fonctionnelle, de risques, ...)\", 0.1790635818929684)\n",
      "('Rééducation nutritionnelle', 0.17435635522005638)\n",
      "('Méthodes de contrôle dans le domaine fonctionnel', 0.17351227588858983)\n"
     ]
    }
   ],
   "source": [
    "# and the top 10 results\n",
    "print(*sorted(zip(corpus.sentences, cosines), key=lambda s: s[1], reverse=True)[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ca90d-9fed-4e53-88d9-c02ef553d661",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "_This did not go well !_\n",
    "\n",
    "Because we are using n-grams to compare and identify words, partial matches can overcome full matches.\n",
    "\n",
    "In the last example, _'relationnelles'_ got stemmed in _'relationnel'_ which got partial\n",
    "matches against _'informationnelles'_ and _'fonctionnelles'_ and ultimately lead to these results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad3bd53-0bc5-4b94-927a-ffb500b9167b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "Now, let's use Gensim to create and train a Word2Vec model on our library of skills.\n",
    "\n",
    "Then, we will use the Word Mover's Distance, which is useful to compare sentences with \n",
    "a Word2Vec model, to compute the closest skill in the library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19288ac-89af-4729-b1e2-ba4a80ac426d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5eac7bdc-8c5d-4a5c-ba81-0f79015ab5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus.tokenized_documents, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16c09b9f-6ba1-4287-8ebc-00cb686a816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between skill_1 and skill_2: 1.3841\n",
      "Distance between skill_2 and skill_3: 1.4250\n",
      "Distance between skill_3 and skill_1: 1.4885\n"
     ]
    }
   ],
   "source": [
    "# compute some distances\n",
    "skill_1 = corpus.clean_sentence(\"Faire un bilan comptable\").split()\n",
    "skill_2 = corpus.clean_sentence(\"Façonner une pâte en matériaux de synthèse\").split()\n",
    "skill_3 = corpus.clean_sentence(\"Peinture sur glace et supports exotiques\").split()\n",
    "\n",
    "print(f\"Distance between skill_1 and skill_2: {model.wv.wmdistance(skill_1, skill_2):.4f}\")\n",
    "print(f\"Distance between skill_2 and skill_3: {model.wv.wmdistance(skill_2, skill_3):.4f}\")\n",
    "print(f\"Distance between skill_3 and skill_1: {model.wv.wmdistance(skill_3, skill_1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0fd537f2-e151-4dbc-8014-ca49a227442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.array([model.wv.wmdistance(skill_1, skill) for skill in corpus.tokenized_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7afba3f-54f5-4fc5-afac-bef76f44c33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gestion comptable', 0.6537737846374512)\n",
      "('Audit comptable', 0.665519118309021)\n",
      "('Écriture comptable', 0.6878059506416321)\n",
      "(\"Règlementation des professionnels de l'expertise comptable\", 0.8841224256981766)\n",
      "('Analyse comptable et financière', 0.8908044811754542)\n",
      "('Gestion comptable et financière des collectivités locales', 0.9298514443611623)\n",
      "('Préparations culinaires de base', 1.0910216569900513)\n",
      "('Plats cuisinés à base de poisson', 1.0910216569900513)\n",
      "('Produits détaxés', 1.1095918416976929)\n",
      "('Produits animaliers', 1.1095918416976929)\n"
     ]
    }
   ],
   "source": [
    "# and the top 10 results\n",
    "# we are using distances here so no need to reverse the argsort\n",
    "print(*list(sorted(zip(corpus.sentences, distances), key=lambda s: s[1]))[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4025bf2a-43f3-4c40-b38c-e4f1cdf63184",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### With Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1d03650-ddbd-437b-84a6-44aae3d336a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_stem.tokenized_documents, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96d63b28-efbf-4ab4-b846-350e073e27b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between skill_1 and skill_2: 1.1868\n",
      "Distance between skill_2 and skill_3: 1.3110\n",
      "Distance between skill_3 and skill_1: 1.3595\n"
     ]
    }
   ],
   "source": [
    "# compute some distances\n",
    "skill_1 = corpus_stem.clean_sentence(\"Faire un bilan comptable\").split()\n",
    "skill_2 = corpus_stem.clean_sentence(\"Façonner une pâte en matériaux de synthèse\").split()\n",
    "skill_3 = corpus_stem.clean_sentence(\"Peinture sur glace et supports exotiques\").split()\n",
    "\n",
    "print(f\"Distance between skill_1 and skill_2: {model.wv.wmdistance(skill_1, skill_2):.4f}\")\n",
    "print(f\"Distance between skill_2 and skill_3: {model.wv.wmdistance(skill_2, skill_3):.4f}\")\n",
    "print(f\"Distance between skill_3 and skill_1: {model.wv.wmdistance(skill_3, skill_1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce2d70ab-b5fe-4ac0-99ec-664efdd447e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distances for skill_3\n",
    "distances = np.array([model.wv.wmdistance(skill_3, skill) for skill in corpus_stem.tokenized_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bbd75031-df15-401b-822d-09227ed1555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Caractéristiques des peintures', 0.6085596770170928)\n",
      "('Techniques de pulvérisation de peinture électrostatique', 0.6142423673368693)\n",
      "('Techniques de peinture à la taloche', 0.6142423673368693)\n",
      "('Techniques de peinture', 0.6142423673368693)\n",
      "(\"Techniques de peinture à l'essuyé\", 0.6142423673368693)\n",
      "('Techniques de peinture à la brosse', 0.6142423673368693)\n",
      "('Peinture sur verre', 0.6278082578882576)\n",
      "('Supports de manutention', 0.6551567396755815)\n",
      "('Types de support de câbles', 0.6778903522794844)\n",
      "('Types de supports audio', 0.6778903522794844)\n"
     ]
    }
   ],
   "source": [
    "# and the top 10 results\n",
    "# we are using distances here so no need to reverse the argsort\n",
    "print(*sorted(zip(corpus_stem.sentences, distances), key=lambda s: s[1])[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93641c6-5a1d-4df3-bb02-3a8652a8bee7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Adding Tf-Idf\n",
    "\n",
    "Let's go a step further.\n",
    "\n",
    "Word2Vec is a model used for finding embeddings for words according to their context.\n",
    "Here, we will be dealing with sentences or paragraphs ...\n",
    "\n",
    "How should we define the embedding of a sentence according to Word2Vec ?\n",
    "\n",
    "There are different methods to get the sentence vectors :\n",
    "- Doc2Vec : we can train your dataset using Doc2Vec and then use the sentence vectors.\n",
    "- Average of Word2Vec vectors : we can average all the word vectors in a sentence to represent the sentence.\n",
    "- Average of Word2Vec vectors with TF-IDF : take the average of word vectors weighted with their TF-IDF scores.\n",
    "\n",
    "We will try the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "265cade7-7ecf-421b-9a1e-8d17ea9d25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a classic Tf-Idf training and transforming\n",
    "tfidf = TfidfVectorizer(lowercase=False, vocabulary=corpus_stem.vocab)\n",
    "tf_mat = tfidf.fit_transform(corpus_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b29d6e8-27f8-40ee-8ea9-f335bc1b9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use min_count=1 to keep every word from the vocabulary\n",
    "model = Word2Vec(\n",
    "    corpus_stem.tokenized_documents,\n",
    "    min_count=1,\n",
    "    vector_size=50,\n",
    "    epochs=100,\n",
    "    workers=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "300f723d-00b0-48b0-8ac2-47a538f41165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3968, 3259)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tf_idf matrix is of shape nb_sentences * len_vocab\n",
    "tf_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9af50413-fad7-48d8-9891-e22590d66f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3259, 50)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the matrix of vectors of the word2vec model is of shape len_vocab * vector_size\n",
    "# however, the vocabulary is not ordered as it is in tf-idf\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b532038a-00f8-424f-b7c0-155dc15b3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every word in ordered vocab (tfidf.get_feature_names), we stack its vector\n",
    "vecs = np.array([model.wv[word] for word in tfidf.get_feature_names()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93dc66b2-0f73-4e33-b026-eae305aaf5bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3968, 50)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now the only operation left to do is multiply matrixes and divide by the sum \n",
    "# of each line of the tf-idf mattrix to obtain the vector embeddings of our \n",
    "# sentences (of shape n_sentences * vector_size)\n",
    "embeddings = tf_mat @ vecs / tf_mat.sum(axis=1).reshape(-1, 1)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19584749-f298-4440-82e1-3205c45e9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill = \"Organiser des évènements officiels et professionnels\"\n",
    "\n",
    "# we need to cast the csr matrix to a coo format to use efficiently columns and data\n",
    "idfs = tfidf.transform([corpus_stem.clean_sentence(skill)]).tocoo()\n",
    "vec = np.sum([idf * vecs[index] for index, idf in zip(idfs.col, idfs.data)], axis=0) / idfs.getnnz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9ead0d9-c4cd-4d46-abd8-85676f9ad310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we compute the cosine similarities with every sentences by using the embedding matrix\n",
    "cosines = cosine_similarity(embeddings, vec.reshape(1, -1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c85156b7-030e-409d-b34b-6ef9f175a739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Organisation d'évènements d'entreprises\", 0.9299365646376524)\n",
      "('Organisation du marché des transports (tarifs, tendances, ...)', 0.9267802473526286)\n",
      "('Organisation du système sanitaire et social', 0.9194771603562804)\n",
      "('Organisation de la chaîne de transport national et international', 0.9163438947544709)\n",
      "('Organisation du système scolaire', 0.9081117240020357)\n",
      "(\"Organisation d'évènements culturels\", 0.9064348731974154)\n",
      "(\"Organisation d'une baie de brassage\", 0.9059132872695261)\n",
      "(\"Organisation d'évènements\", 0.9052304716102239)\n",
      "('Organisation de soirées', 0.9047534173617487)\n",
      "('Sociologie des organisations', 0.9037087604696366)\n"
     ]
    }
   ],
   "source": [
    "# and the top 10 results\n",
    "print(*sorted(zip(corpus_stem.sentences, cosines), key=lambda s: s[1], reverse=True)[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00da888-b59a-4fe0-9ab4-9d731ae62c29",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Spacy and pre-trained models\n",
    "\n",
    "Finally, we will test some models from Spacy, because we can and it is easier.\n",
    "\n",
    "If everything goes well, this could be better than everything we tried so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c87085ae-641c-4bfd-9282-1414d139a5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to download the pre trained model for french documents, uncomment the following line\n",
    "# !spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "901d5a03-2b6f-4ec0-abf9-29a4c2a16b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f45782f-7224-4e52-8960-64c898c88cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill = nlp(\"Administrer des bases de données\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed18f647-621e-41e4-9d57-985f9eb9d210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-aa2290f2a557>:2: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similarity = np.array([skill.similarity(nlp(doc)) for doc in corpus.sentences])\n"
     ]
    }
   ],
   "source": [
    "# build the array of similarities\n",
    "similarity = np.array([skill.similarity(nlp(doc)) for doc in corpus.sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "59030f88-034e-4ace-a4e3-05b06e304a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Règles de traitement des opérations bancaires', 0.9264853175083053)\n",
      "('Méthodes de valorisation des stocks', 0.9218128374642741)\n",
      "('Fonctionnement des matériels de reprographie', 0.9176832957531333)\n",
      "('Gestion de bases de données', 0.9171529300387925)\n",
      "('Caractéristiques des dossiers de maintenance', 0.9144608210200109)\n",
      "('Méthodes de contrôle de structure des matériaux', 0.9140826994088761)\n",
      "('Procédures de maintenance des équipements de décontamination', 0.9123576073628312)\n",
      "('Fonctionnement des machines de développement automatisé', 0.910774109408425)\n",
      "('Méthodes de Gestion des Moyens de Production', 0.9093153749549872)\n",
      "('Outils de planification des ressources humaines', 0.9086192583382122)\n"
     ]
    }
   ],
   "source": [
    "# see the top 10 skills\n",
    "print(*sorted(zip(corpus.sentences, similarity), key=lambda s: s[1], reverse=True)[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d9398-f948-4770-b6fc-6c9a23b7fc8d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Extracting skills from a resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7509dadd-3808-4005-b38c-9818d226282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pyMuPDF with fitz to read PDFs\n",
    "doc = fitz.open(\"CV_test/CV_1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "42717cbf-8378-4946-94a1-fda5bbe473d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extract each block from the PDF individually\n",
    "# we split sentences and store them in the following list\n",
    "sentences = []\n",
    "for block in doc.loadPage(0).get_text(\"blocks\"):\n",
    "    block_text = sub(r\"<[^>]*>|\\n\", \" \", block[4])\n",
    "    for st in split(r\"[.!?-]\\s\", block_text):\n",
    "        if cst := st.strip():\n",
    "            sentences.append(cst)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a4c009ec-9ab5-4ad9-8252-0cf1b3d792ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a Word2Vec model with the Tf-Idf subtlety as explained above\n",
    "tfidf = TfidfVectorizer(lowercase=False, vocabulary=corpus_stem.vocab)\n",
    "model = Word2Vec(\n",
    "    corpus_stem.tokenized_documents,\n",
    "    min_count=1,\n",
    "    vector_size=50,\n",
    "    epochs=100,\n",
    "    workers=10,\n",
    ")\n",
    "\n",
    "tf_mat = tfidf.fit_transform(corpus_stem)\n",
    "vecs = np.array([model.wv[word] for word in tfidf.get_feature_names()])\n",
    "embeddings = tf_mat @ vecs / tf_mat.sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4419f4bc-bc35-46d9-af96-a48c174232ba",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Extract best skill per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0eedbb1-d1ef-476e-a305-729797982f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sentence(sentence: str) -> tuple:\n",
    "    \"\"\"Compare a sentence with the corpus and returns the best match\"\"\"\n",
    "    \n",
    "    # idf scores for the input sentence\n",
    "    # we use COO format to access efficiently indexes and data\n",
    "    tfidfs = tfidf.transform([corpus_stem.clean_sentence(sentence)]).tocoo()\n",
    "    \n",
    "    # if no word in the sentence matches the known vocabulary, don't bother\n",
    "    if tfidfs.getnnz() == 0:\n",
    "        return (\"\", 0)\n",
    "    \n",
    "    # we compute the average of the words embeddings from the input sentence\n",
    "    # weighted with their tf-idf scores\n",
    "    vec = np.sum([tfidf * vecs[index] for index, tfidf in zip(tfidfs.col, tfidfs.data)], axis=0) / tfidfs.sum()\n",
    "    \n",
    "    # then we compute similarities with the corpus' embeddings\n",
    "    cosines = cosine_similarity(embeddings, vec.reshape(1, -1)).flatten()\n",
    "    \n",
    "    # and return the best match\n",
    "    return max(zip(corpus_stem.sentences, cosines), key=lambda s: s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f2c8b380-ad28-4766-aec6-8c49344ca246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Marketing des réseaux sociaux', 0.9832577476846847)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_sentence('Gestionnaire de réseaux sociaux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "07fc45fb-b24b-40ee-952f-77f1d4332859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compare every sentence extracted from the resume with the skills corpus\n",
    "# and extract the best matching skill\n",
    "resume_skills = [compare_sentence(st) for st in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5fbb304-ab67-4475-ad74-accdc2e78eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Marketing des réseaux sociaux', 0.999999999999999),\n",
       " ('Insights marketing', 0.9902524637145232),\n",
       " (\"Équipements d'imprimerie\", 0.9890709084053457),\n",
       " ('Marketing des réseaux sociaux', 0.9832577476846847),\n",
       " ('Marketing des réseaux sociaux', 0.9832577476846847),\n",
       " ('Communication interpersonnelle', 0.9789137887522529),\n",
       " ('Marketing des réseaux sociaux', 0.9757376356113969),\n",
       " (\"Gouvernance d'entreprise\", 0.9640695292837177),\n",
       " ('Actualité quotidienne / informations', 0.9614404753259805),\n",
       " ('Hospitalisation à domicile', 0.9612038974944008)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(resume_skills, key=lambda s: s[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f35bbae8-0e46-4b27-834d-a1289818c72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Marketing des réseaux sociaux', 0.999999999999999),\n",
       " ('Insights marketing', 0.9902524637145232),\n",
       " (\"Équipements d'imprimerie\", 0.9890709084053457),\n",
       " ('Communication interpersonnelle', 0.9789137887522529),\n",
       " (\"Gouvernance d'entreprise\", 0.9640695292837177),\n",
       " ('Actualité quotidienne / informations', 0.9614404753259805),\n",
       " ('Hospitalisation à domicile', 0.9612038974944008),\n",
       " ('Émissions avec débats', 0.9578283264070274),\n",
       " ('Communication digitale', 0.9563800261157076),\n",
       " ('Ingénierie de la formation', 0.9557226875014917)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique skills\n",
    "sorted(dict(sorted(resume_skills, key=lambda s: s[1])).items(), key=lambda s: s[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74199bb-cb2d-47f4-9a94-8db586499ba7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Not bad but we need a way to combine and take into account skills that were matched more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419141da-7040-46cd-971c-e71086e527b7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Extract best skills overall\n",
    "\n",
    "Instead of extracting the best match for each sentence in the resume,\n",
    "we could average the distances of the sentences to each skill in the corpus\n",
    "and then extract the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f31b1f7b-4488-4b2a-b1de-2a0e66c06ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_distances = np.zeros((embeddings.shape[0], 1))\n",
    "sentence_count = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    # idf scores for the input sentence (as previously)\n",
    "    # we use COO format to access efficiently indexes and data\n",
    "    tfidfs = tfidf.transform([corpus_stem.clean_sentence(sentence)]).tocoo()\n",
    "    \n",
    "    # this time, if no word in the sentence matches the known vocabulary, just skip\n",
    "    if tfidfs.getnnz() == 0:\n",
    "        continue\n",
    "        \n",
    "    # compute the weighted average with tf-idf scores\n",
    "    vec = np.sum([tfidf * vecs[index] for index, tfidf in zip(tfidfs.col, tfidfs.data)], axis=0) / tfidfs.sum()\n",
    "    \n",
    "    # compute similarities with the corpus\n",
    "    cosines = cosine_similarity(embeddings, vec.reshape(1, -1))\n",
    "    \n",
    "    # and this time, we add the similarities to the resume_distances array\n",
    "    resume_distances += cosines\n",
    "    # and add one to the sentence count\n",
    "    sentence_count += 1\n",
    "    \n",
    "    \n",
    "# eventually, we divide resume_distances by the number of sentences in the resume\n",
    "resume_distances /= (sentence_count or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9fb6344b-be02-4541-9786-bfc0a5d625ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Localisation de panne', 0.7597689023855665),\n",
       " ('Diagnostic capillaire', 0.7526106529920145),\n",
       " ('Aide au maintien à domicile', 0.7403831347612659),\n",
       " ('UX/UI Design', 0.7377252575912373),\n",
       " ('Microsoft Office PowerPoint (diaporama)', 0.7363804169962034),\n",
       " ('Hospitalisation à domicile', 0.7263621168744556),\n",
       " ('Reporting anglo-saxon', 0.7187427003846412),\n",
       " ('Nature morte', 0.71769451397857),\n",
       " ('Responsabilité Sociétale des Entreprises (RSE)', 0.7173649919877738),\n",
       " ('Généalogie', 0.7149018007722692)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(corpus_stem.sentences, resume_distances[:, 0]), key=lambda s: s[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d099b-dfc9-4b4f-b954-b3c2e0f04184",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Obviously, this didn't work.\n",
    "\n",
    "The reason is that some weak matches occurring frequently can overcome strong matches\n",
    "occurring infrequently.\n",
    "\n",
    "Moreover, if the resume presents two orthogonal skills or set of skills (algebraically speaking),\n",
    "they can counterinteract when averaging.  \n",
    "Detecting a skill with 100% confidence in a sentence\n",
    "will result in a 0% confidence for the orthogonal skills and inversely, thus averaging to 50%\n",
    "over the whole resume ...)\n",
    "\n",
    "A more accurate method would be to build a competitive method, as in sporting events where \n",
    "teams gather points in each category according to their ranking and the overall winner is the \n",
    "team with the most points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb06c999-190e-4b6f-af34-584fc5aa3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_matches(sentence: str, n_best: int) -> tuple:\n",
    "    \"\"\"Compare a sentence with the corpus and returns the n best matches\"\"\"\n",
    "    \n",
    "    # idf scores for the input sentence\n",
    "    # we use COO format to access efficiently indexes and data\n",
    "    tfidfs = tfidf.transform([corpus_stem.clean_sentence(sentence)]).tocoo()\n",
    "    \n",
    "    # if no word in the sentence matches the known vocabulary, don't bother\n",
    "    if tfidfs.getnnz() == 0:\n",
    "        return None\n",
    "    \n",
    "    # we compute the average of the words embeddings from the input sentence\n",
    "    # weighted with their tf-idf scores\n",
    "    vec = np.sum([tfidf * vecs[index] for index, tfidf in zip(tfidfs.col, tfidfs.data)], axis=0) / tfidfs.sum()\n",
    "    \n",
    "    # then we compute similarities with the corpus' embeddings\n",
    "    cosines = cosine_similarity(embeddings, vec.reshape(1, -1)).flatten()\n",
    "    \n",
    "    # and return the best match\n",
    "    return sorted(zip(corpus_stem.sentences, cosines), key=lambda s: s[1])[:-n_best - 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d7a7e29-8ec0-4fa8-89dc-988acdd2be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary of skills encountered and the points associated\n",
    "skills_rankings = {}\n",
    "n_best = 3\n",
    "\n",
    "for sentence in sentences:\n",
    "    \n",
    "    # get best matches\n",
    "    top = extract_best_matches(sentence, n_best)\n",
    "    \n",
    "    # if the sentence could not be matched, continue\n",
    "    if top is None:\n",
    "        continue\n",
    "    \n",
    "    # we allow points logarithmically from 1 to 2**(n_best - 1)\n",
    "    # these points are weighted with the cosine associated\n",
    "    # and then added to the current points of the skill in the ranking list\n",
    "    for skill, points in zip(top, np.logspace(0, n_best-1, num=n_best, base=2)[::-1]):\n",
    "        skills_rankings[skill[0]] = skills_rankings.get(skill[0], 0) + points * skill[1]\n",
    "        \n",
    "        \n",
    "# eventually, we sort the skills and normalize each scores with the maximum score\n",
    "max_score = max(skills_rankings.values())\n",
    "rankings = sorted(skills_rankings.items(), key=lambda s: s[1], reverse=True)[:10]\n",
    "rankings = list(map(lambda s: (s[0], s[1] / max_score), rankings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c74c5381-f4db-4dbc-88cb-8090decbe34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Marketing des réseaux sociaux', 1.0),\n",
       " ('Émissions avec débats', 0.48592938838949495),\n",
       " ('Ingénierie de la formation', 0.4848611470383812),\n",
       " ('Diagnostic capillaire', 0.4556705090121022),\n",
       " ('Insights marketing', 0.25118946724462754),\n",
       " (\"Équipements d'imprimerie\", 0.25088975150595716),\n",
       " ('Communication interpersonnelle', 0.2483132757405448),\n",
       " ('Réseaux Digital Subscriber Line (DSL)', 0.24454895108055213),\n",
       " (\"Gouvernance d'entreprise\", 0.24454785049377933),\n",
       " ('Gestuelle de communication avec le personnel de piste',\n",
       "  0.24406472333275903)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see results\n",
    "rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7266146-56c2-4706-b6fb-eb64b1073bee",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "To be continued ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
